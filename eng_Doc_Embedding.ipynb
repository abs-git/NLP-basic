{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Doc_Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPwAx5RCUhNy42C1OypXW3q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/English_Doc_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_5hrEdsrEuU",
        "outputId": "9c6150bb-c5cd-4fc7-85ac-4b5f9140da48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# glue data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(\"train : {}\".format(len(train_sentences)))\n",
        "print(\"test : {}\".format(len(test_sentences)))\n",
        "\n",
        "print(train_sentences[:2])\n",
        "print(test_sentences[:2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ8sK_L2rLvO",
        "outputId": "0c29e9e3-adbf-4335-dfd4-4f57a3f2584e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 8551\n",
            "test : 1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\"]\n",
            "['Bill whistled past the house.', 'The car honked its way down the road.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency based embedding"
      ],
      "metadata": {
        "id": "1aH7uwtd9maK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSA (Latent Semantic Analysis)"
      ],
      "metadata": {
        "id": "CsPwT-Eg-Ie9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSA (Latent Semantic Analysis, 잠재 의미 분석)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "\n",
        "sens = []\n",
        "for sen in train_sentences[:100]:\n",
        "  sens.append(sen.lower())\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features = 100, max_df = 0.5, smooth_idf = True)\n",
        "\n",
        "X = vectorizer.fit_transform(sens)\n",
        "\n",
        "words = vectorizer.get_feature_names()\n",
        "\n",
        "print(X.shape)  # (Number of sentence, Number of words)\n",
        "\n",
        "svd = TruncatedSVD(n_components=20, algorithm= 'randomized', n_iter = 10, random_state = 100)\n",
        "\n",
        "svd.fit(X)\n",
        "\n",
        "print(svd.components_.shape)  # V_t의 shape (특이값, vector size)\n",
        "\n",
        "k = 10\n",
        "components = svd.components_\n",
        "for idx, topic in enumerate(components):\n",
        "  top_k_topics = [words[i] for i in topic.argsort()[: -k -1 : -1]]\n",
        "\n",
        "  print(top_k_topics)\n",
        "\n"
      ],
      "metadata": {
        "id": "bljGGBjurSNs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2dfd24-1a48-4324-b749-47bcd7b161c3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 100)\n",
            "(20, 100)\n",
            "['coughed', 'harry', 'awake', 'fit', 'pushed', 'yelled', 'nose', 'hours', 'john', 'hoarse']\n",
            "['building', 'taller', 'got', 'wider', 'tall', 'free', 'sam', 'replacement', 'cried', 'fred']\n",
            "['road', 'forest', 'rumbled', 'wagon', 'witch', 'vanished', 'went', 'vanishing', 'tunnel', 'trolley']\n",
            "['yelled', 'hoarse', 'harry', 'hours', 'pushed', 'trail', 'floated', 'cave', 'river', 'day']\n",
            "['sleep', 'cried', 'sue', 'sang', 'fred', 'verbs', 'study', 'tracked', 'source', 'bled']\n",
            "['loose', 'wriggled', 'tooth', 'carpet', 'worm', 'cried', 'bled', 'day', 'tiger', 'angry']\n",
            "['floated', 'hours', 'pushed', 'harry', 'cave', 'river', 'trail', 'fit', 'sang', 'want']\n",
            "['pulley', 'rope', 'stretched', 'weights', 'cried', 'day', 'sd', 'sf', 'flowers', 'gardener']\n",
            "['flat', 'metal', 'watered', 'gardener', 'flowers', 'fred', 'source', 'tracked', 'cried', 'day']\n",
            "['drank', 'pub', 'cried', 'bled', 'tiger', 'day', 'fred', 'way', 'angry', 'sf']\n",
            "['floated', 'cave', 'river', 'awake', 'yelled', 'john', 'nose', 'hoarse', 'play', 'hours']\n",
            "['forest', 'witch', 'vanished', 'went', 'vanishing', 'cried', 'bled', 'tiger', 'river', 'fred']\n",
            "['room', 'dog', 'barked', 'way', 'rolled', 'past', 'house', 'whistled', 'waltzes', 'john']\n",
            "['floor', 'bled', 'toilet', 'tiger', 'fred', 'source', 'tracked', 'want', 'river', 'ring']\n",
            "['president', 'making', 'caused', 'angry', 'broke', 'bathtub', 'verbs', 'study', 'sd', 'sf']\n",
            "['broke', 'bathtub', 'generalization', 'giving', 'pseudo', 'want', 'fred', 'source', 'tracked', 'cried']\n",
            "['generalization', 'giving', 'pseudo', 'angry', 'making', 'caused', 'president', 'cried', 'day', 'replacement']\n",
            "['free', 'sam', 'got', 'taller', 'wider', 'replacement', 'bled', 'study', 'verbs', 'president']\n",
            "['house', 'past', 'whistled', 'way', 'restaurant', 'coughed', 'window', 'fit', 'floated', 'cave']\n",
            "['tall', 'sam', 'free', 'building', 'got', 'rumbled', 'want', 'fred', 'trolley', 'tunnel']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA (Latent Dirichlet Allocation)"
      ],
      "metadata": {
        "id": "rM_RnlnV-SFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LDA (Latent Dirichlet Allocation, 잠재 디리클레 할당)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "\n",
        "# 불용어 제거, 토큰화\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "sen_to_tokens = []\n",
        "for sen in train_sentences[:1000]:\n",
        "  sen = sen.lower()\n",
        "  tokens = nltk.word_tokenize(sen)\n",
        "\n",
        "  temp = []\n",
        "  for token in tokens:\n",
        "    if token not in stop_words:\n",
        "      temp.append(token)\n",
        "\n",
        "  sen_to_tokens.append(temp)\n",
        "\n",
        "print(sen_to_tokens[:5])\n",
        "\n",
        "\n",
        "# 역 토큰화 후 TF-IDF\n",
        "detokenized = []\n",
        "for tokens in sen_to_tokens:\n",
        "  sen = ' '.join(tokens)\n",
        "  detokenized.append(sen)\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 1000)\n",
        "tf_idf_mat = vectorizer.fit_transform(detokenized)\n",
        "\n",
        "print(tf_idf_mat.shape)\n",
        "\n",
        "\n",
        "# LDA\n",
        "\n",
        "nTopics = 5\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components = nTopics, learning_method='online', random_state = 100, max_iter = 1)\n",
        "lda_topics = lda.fit_transform(tf_idf_mat)\n",
        "\n",
        "print(lda.components_.shape)\n",
        "\n",
        "terms_names = vectorizer.get_feature_names()\n",
        "top_n = 5\n",
        "for idx, topic in enumerate(lda.components_):\n",
        "\n",
        "  top_list = []\n",
        "  for i in topic.argsort()[:-top_n -1 : -1]:\n",
        "    top_list.append((terms_names[i], round(topic[i], 2)))\n",
        "\n",
        "  print(top_list)\n"
      ],
      "metadata": {
        "id": "6qlLpK2Z9tuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bc96f01-3273-488d-cb49-990c7389bfe4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['friends', 'wo', \"n't\", 'buy', 'analysis', ',', 'let', 'alone', 'next', 'one', 'propose', '.'], ['one', 'pseudo', 'generalization', \"'m\", 'giving', '.'], ['one', 'pseudo', 'generalization', \"'m\", 'giving', '.'], ['study', 'verbs', ',', 'crazier', 'get', '.'], ['day', 'day', 'facts', 'getting', 'murkier', '.']]\n",
            "(1000, 996)\n",
            "(5, 996)\n",
            "[('mary', 22.95), ('john', 8.7), ('eat', 7.82), ('think', 5.97), ('likes', 5.56)]\n",
            "[('sally', 8.38), ('love', 6.86), ('problem', 5.61), ('ll', 4.43), ('joe', 4.06)]\n",
            "[('john', 5.69), ('water', 5.19), ('leave', 4.14), ('baby', 4.01), ('cup', 3.97)]\n",
            "[('john', 24.12), ('mary', 12.84), ('book', 10.86), ('ball', 7.94), ('left', 7.24)]\n",
            "[('consider', 5.19), ('man', 5.05), ('talks', 4.34), ('garden', 4.21), ('lobbyists', 4.18)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction based embedding"
      ],
      "metadata": {
        "id": "BH7cN5wq-Z-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sent2Vec (Sentence to vector)"
      ],
      "metadata": {
        "id": "FYdDvaEk-Y_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Doc2Vec (Document to vector)"
      ],
      "metadata": {
        "id": "g0Rkt_87-tll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

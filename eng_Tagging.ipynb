{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Tagging.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMsQalhjmWbNNhhX3kbnQxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/Tagging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtMzMcxvJhQt",
        "outputId": "9dc7e3b6-5bb4-47f1-e0f0-e138142480b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# glue data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(\"train : {}\".format(len(train_sentences)))\n",
        "print(\"test : {}\".format(len(test_sentences)))\n",
        "\n",
        "print(train_sentences[:2])\n",
        "print(test_sentences[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxyT9zzJQxbA",
        "outputId": "50425716-7198-4eba-8982-ff9fee389d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 8551\n",
            "test : 1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\"]\n",
            "['Bill whistled past the house.', 'The car honked its way down the road.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "sample_sentences = []\n",
        "for sen in train_sentences[:2]:\n",
        "  words = sen.split(\" \")\n",
        "\n",
        "  temp = []\n",
        "  for w in words:\n",
        "    temp.append(w)\n",
        "\n",
        "  sample_sentences.append(temp)\n"
      ],
      "metadata": {
        "id": "CIIKpqZp02uJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nltk built-in tagger\n",
        "import nltk\n",
        "nltk.download('treebank')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq99BF5q0_t0",
        "outputId": "1dc36720-9a9b-40be-85f8-8410ce61ac07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of Speech Tagging\n",
        "\n",
        "from nltk.corpus import treebank\n",
        "# from nltk.corpus.reader import bnc\n",
        "\n",
        "from nltk.tag.sequential import DefaultTagger \n",
        "from nltk.tag.sequential import UnigramTagger\n",
        "from nltk.tag.sequential import BigramTagger, TrigramTagger\n",
        "\n",
        "\n",
        "# nltk built-in tagset reference (corpus,sentence,word,char)\n",
        "treebank_corpus = treebank.tagged_sents()[:3000]          # Penn Treebank tagset (45 cate)\n",
        "# bnc_corpus = bnc.tagged_sents()[:3000]                  # british national tagset (98 cate)\n",
        "\n",
        "# tagger load\n",
        "init_tagger = DefaultTagger('EX')                  # 모든 토큰에 대한 기본 pos를 부여한다.\n",
        "init_tag = init_tagger.tag(sample_sentences[0])\n",
        "\n",
        "uni_tagger = UnigramTagger(treebank_corpus)\n",
        "uni_tag = uni_tagger.tag(sample_sentences[0])\n",
        "\n",
        "uni_backoff_tagger = UnigramTagger(treebank_corpus, backoff = init_tagger)    # Backoff tagger : tagging 하지 못한 token엔 default 값을 부여한다.\n",
        "uni_backoff_tag = uni_backoff_tagger.tag(sample_sentences[0])\n",
        "\n",
        "bi_tagger = BigramTagger(treebank_corpus)              # bigram tagger : 해당 token의 이전 token의 tag를 참고\n",
        "bi_tag = bi_tagger.tag(sample_sentences[0])\n",
        "\n",
        "tri_tagger = TrigramTagger(treebank_corpus)             # trigram tagger : 해당 token의 이전 2개의 token의 tag를 참고\n",
        "tri_tag = tri_tagger.tag(sample_sentences[0])\n",
        "\n",
        "\n",
        "print('raw sentence     : {}'.format(sample_sentences[0]))\n",
        "print('init tag         : {}'.format(init_tag))\n",
        "print('unigram tag      : {}'.format(uni_tag))\n",
        "print('uni backoff tag  : {}'.format(uni_backoff_tag))\n",
        "print('bigram tag       : {}'.format(bi_tag))\n",
        "print('trigram tag      : {}'.format(tri_tag))\n",
        "print()\n",
        "\n",
        "# print(init_tagger.evaluate(sample_sentences[1]))\n"
      ],
      "metadata": {
        "id": "m2VCVcneVMtO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "800ff89c-f613-46ce-830f-8d1187b8d024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw sentence     : ['Our', 'friends', \"won't\", 'buy', 'this', 'analysis,', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose.']\n",
            "init tag         : [('Our', 'EX'), ('friends', 'EX'), (\"won't\", 'EX'), ('buy', 'EX'), ('this', 'EX'), ('analysis,', 'EX'), ('let', 'EX'), ('alone', 'EX'), ('the', 'EX'), ('next', 'EX'), ('one', 'EX'), ('we', 'EX'), ('propose.', 'EX')]\n",
            "unigram tag      : [('Our', None), ('friends', 'NNS'), (\"won't\", None), ('buy', 'VB'), ('this', 'DT'), ('analysis,', None), ('let', 'VB'), ('alone', 'RB'), ('the', 'DT'), ('next', 'JJ'), ('one', 'CD'), ('we', 'PRP'), ('propose.', None)]\n",
            "uni backoff tag  : [('Our', 'EX'), ('friends', 'NNS'), (\"won't\", 'EX'), ('buy', 'VB'), ('this', 'DT'), ('analysis,', 'EX'), ('let', 'VB'), ('alone', 'RB'), ('the', 'DT'), ('next', 'JJ'), ('one', 'CD'), ('we', 'PRP'), ('propose.', 'EX')]\n",
            "bigram tag       : [('Our', None), ('friends', None), (\"won't\", None), ('buy', None), ('this', None), ('analysis,', None), ('let', None), ('alone', None), ('the', None), ('next', None), ('one', None), ('we', None), ('propose.', None)]\n",
            "trigram tag      : [('Our', None), ('friends', None), (\"won't\", None), ('buy', None), ('this', None), ('analysis,', None), ('let', None), ('alone', None), ('the', None), ('next', None), ('one', None), ('we', None), ('propose.', None)]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def backoff_tagger(train_sent, tagger_classes, backoff = None):\n",
        "  for cls in tagger_classes:\n",
        "    backoff = cls(train_sent, backoff = backoff)\n",
        "\n",
        "  return backoff\n",
        "\n",
        "tagger = backoff_tagger(treebank_corpus, [UnigramTagger, BigramTagger, TrigramTagger],\n",
        "                        backoff = DefaultTagger('NN'))\n",
        "\n",
        "backoff_tag = tagger.tag(sample_sentences[0])\n",
        "# tag_score = tagger.evaluate(sample_sentences[0:])\n",
        "\n",
        "print('backoff tag : {}'.format(backoff_tag))\n",
        "# print('accuracy    : {}'.format(tag_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNpjj0GXZT8F",
        "outputId": "8af7ddad-8cd3-4912-e111-bc5653590d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backoff tag : [('Our', 'NN'), ('friends', 'NNS'), (\"won't\", 'NN'), ('buy', 'VB'), ('this', 'DT'), ('analysis,', 'NN'), ('let', 'VBD'), ('alone', 'RB'), ('the', 'DT'), ('next', 'JJ'), ('one', 'CD'), ('we', 'PRP'), ('propose.', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named Entity Recognition\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "id": "1XZZ0mF429TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag, ne_chunk\n",
        "\n",
        "tagged_sentence = pos_tag(sample_sentences[0])    # 다른 tagger를 활용해 tagging 하여도 상관 없다.\n",
        "\n",
        "ner_sentence = ne_chunk(tagged_sentence)\n",
        "\n",
        "print(tagged_sentence)\n",
        "print(ner_sentence)\n"
      ],
      "metadata": {
        "id": "01tVP11alHh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff7533b8-0ada-4c31-b50c-6d112d0fa106"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Our', 'PRP$'), ('friends', 'NNS'), (\"won't\", 'VBP'), ('buy', 'VB'), ('this', 'DT'), ('analysis,', 'JJ'), ('let', 'VB'), ('alone', 'RB'), ('the', 'DT'), ('next', 'JJ'), ('one', 'NN'), ('we', 'PRP'), ('propose.', 'VBP')]\n",
            "(S\n",
            "  Our/PRP$\n",
            "  friends/NNS\n",
            "  won't/VBP\n",
            "  buy/VB\n",
            "  this/DT\n",
            "  analysis,/JJ\n",
            "  let/VB\n",
            "  alone/RB\n",
            "  the/DT\n",
            "  next/JJ\n",
            "  one/NN\n",
            "  we/PRP\n",
            "  propose./VBP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eFwwOHnK26l_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

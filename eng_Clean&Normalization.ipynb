{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Clean&Normalization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPnKqz2ePT6P8+Gvr3Osaty",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/English_Clean%26Normalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VLLuVwpj0gJu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e1408d-f6b4-48eb-c924-de57fc1970e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Load"
      ],
      "metadata": {
        "id": "mFfRTQgkRn7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# glue data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(\"train : {}\".format(len(train_sentences)))\n",
        "print(\"test : {}\".format(len(test_sentences)))\n",
        "\n",
        "print(train_sentences[:2])\n",
        "print(test_sentences[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDzzUqGHNtJV",
        "outputId": "4341e33d-f387-45c3-b6c7-d112d40dda0e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 8551\n",
            "test : 1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\"]\n",
            "['Bill whistled past the house.', 'The car honked its way down the road.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization (정규화)"
      ],
      "metadata": {
        "id": "1MWsnp47RNG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3kYdvHuSoE_",
        "outputId": "fa1149a5-b1e1-4e54-f04e-f2fbca1740a1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming (어간 추출) & Lemmatization (표제어 추출)\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "tokens = word_tokenize(train_sentences[0])\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "stem_tokens = [stemmer.stem(t) for t in tokens]\n",
        "lem_tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "\n",
        "print(\"tokens    : {}\".format(tokens))\n",
        "print(\"stem      : {}\".format(stem_tokens))\n",
        "print(\"lemmatize : {}\".format(lem_tokens))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "729P7rhhN_r3",
        "outputId": "62c29e69-9dc2-4833-a35d-f3c6db22e5b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens    : ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "stem      : ['our', 'friend', 'wo', \"n't\", 'buy', 'thi', 'analysi', ',', 'let', 'alon', 'the', 'next', 'one', 'we', 'propos', '.']\n",
            "lemmatize : ['Our', 'friend', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(lemmatizer.lemmatize('has', 'v'))          # 품사를 지정해주면 정확한 표제어를 얻을 수 있다.\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "def lemma(pos_list):\n",
        "  ''' \n",
        "  Lemmatize 함수가 입력받는 품사는 동사(v), 형용사(a), 명사(n), 부사(r)이다. \n",
        "  nltk.pos_tag로 반환 받는 형용사는 J이기 때문에 Lemmatize를 위해 n으로 pos를 변경한다.\n",
        "  '''\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemma_list = []\n",
        "  for token, pos in pos_list:\n",
        "\n",
        "    if pos[0] in ['V', 'J', 'N', 'R']:\n",
        "\n",
        "      if pos[0] == 'J':\n",
        "        pos = 'a'\n",
        "      else :\n",
        "        pos = pos.lower()\n",
        "\n",
        "      lemma_token = lemmatizer.lemmatize(token, pos[0])      \n",
        "      lemma_list.append(lemma_token)\n",
        "\n",
        "  return lemma_list\n",
        "\n",
        "\n",
        "pos_tokens = pos_tag(tokens)\n",
        "\n",
        "my_lemm_tokens = lemma(pos_tokens)\n",
        "\n",
        "print(pos_tokens)\n",
        "print(my_lemm_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GLatmuaVfzN",
        "outputId": "fb977ba4-527a-4261-d995-12a4e088713a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Our', 'PRP$'), ('friends', 'NNS'), ('wo', 'MD'), (\"n't\", 'RB'), ('buy', 'VB'), ('this', 'DT'), ('analysis', 'NN'), (',', ','), ('let', 'VB'), ('alone', 'RB'), ('the', 'DT'), ('next', 'JJ'), ('one', 'NN'), ('we', 'PRP'), ('propose', 'VBP'), ('.', '.')]\n",
            "['friend', \"n't\", 'buy', 'analysis', 'let', 'alone', 'next', 'one', 'propose']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean (정제)"
      ],
      "metadata": {
        "id": "cDx4Pbj4eDgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean (Stopword) / After lemmatization\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "no_stopword = []\n",
        "for t in lem_tokens:\n",
        "  if t not in stop_words:\n",
        "    no_stopword.append(t)\n",
        "\n",
        "print('lemma tokens     : {}'.format(lem_tokens))\n",
        "print('removed stopword : {}'.format(no_stopword))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_v606yILN5GN",
        "outputId": "01f8379d-e3be-4f8a-c9c3-d8892d6357a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma tokens     : ['Our', 'friend', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "removed stopword : ['Our', 'friend', 'wo', \"n't\", 'buy', 'analysis', ',', 'let', 'alone', 'next', 'one', 'propose', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6go7doxkdXKY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

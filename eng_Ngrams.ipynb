{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Ngrams.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZ7RCWri1Bmz560f387vgk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/English_Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU3TJGQYC5U0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4874bc65-9252-495e-a1af-c30c67115416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# glue data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(\"train : {}\".format(len(train_sentences)))\n",
        "print(\"test : {}\".format(len(test_sentences)))\n",
        "\n",
        "print(train_sentences[:2])\n",
        "print(test_sentences[:2])\n"
      ],
      "metadata": {
        "id": "K2j5OHrfDC_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf61e9f-f254-4d27-82fd-cd3e853cdbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 8551\n",
            "test : 1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\"]\n",
            "['Bill whistled past the house.', 'The car honked its way down the road.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "yznWeRcEDEdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41091926-f21c-4099-c3f9-1cfbde695c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch ngram\n",
        "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "tokens = []\n",
        "for sen in train_sentences[:3]:\n",
        "  sen_to_tokens = tokenizer(sen)\n",
        "  tokens.extend(sen_to_tokens)\n",
        "\n",
        "\n",
        "two_grams = list(ngrams_iterator(tokens, 2))\n",
        "three_grams = list(ngrams_iterator(tokens, 3))\n",
        "\n",
        "\n",
        "print(two_grams)\n",
        "print(three_grams)\n"
      ],
      "metadata": {
        "id": "sTNwEMgFDGtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4f85362-165a-4704-a021-da6f36449f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', 'one', 'more', 'pseudo', 'generalization', 'and', 'i', \"'\", 'm', 'giving', 'up', '.', 'one', 'more', 'pseudo', 'generalization', 'or', 'i', \"'\", 'm', 'giving', 'up', '.', 'our friends', 'friends won', \"won '\", \"' t\", 't buy', 'buy this', 'this analysis', 'analysis ,', ', let', 'let alone', 'alone the', 'the next', 'next one', 'one we', 'we propose', 'propose .', '. one', 'one more', 'more pseudo', 'pseudo generalization', 'generalization and', 'and i', \"i '\", \"' m\", 'm giving', 'giving up', 'up .', '. one', 'one more', 'more pseudo', 'pseudo generalization', 'generalization or', 'or i', \"i '\", \"' m\", 'm giving', 'giving up', 'up .']\n",
            "['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', 'one', 'more', 'pseudo', 'generalization', 'and', 'i', \"'\", 'm', 'giving', 'up', '.', 'one', 'more', 'pseudo', 'generalization', 'or', 'i', \"'\", 'm', 'giving', 'up', '.', 'our friends', 'friends won', \"won '\", \"' t\", 't buy', 'buy this', 'this analysis', 'analysis ,', ', let', 'let alone', 'alone the', 'the next', 'next one', 'one we', 'we propose', 'propose .', '. one', 'one more', 'more pseudo', 'pseudo generalization', 'generalization and', 'and i', \"i '\", \"' m\", 'm giving', 'giving up', 'up .', '. one', 'one more', 'more pseudo', 'pseudo generalization', 'generalization or', 'or i', \"i '\", \"' m\", 'm giving', 'giving up', 'up .', 'our friends won', \"friends won '\", \"won ' t\", \"' t buy\", 't buy this', 'buy this analysis', 'this analysis ,', 'analysis , let', ', let alone', 'let alone the', 'alone the next', 'the next one', 'next one we', 'one we propose', 'we propose .', 'propose . one', '. one more', 'one more pseudo', 'more pseudo generalization', 'pseudo generalization and', 'generalization and i', \"and i '\", \"i ' m\", \"' m giving\", 'm giving up', 'giving up .', 'up . one', '. one more', 'one more pseudo', 'more pseudo generalization', 'pseudo generalization or', 'generalization or i', \"or i '\", \"i ' m\", \"' m giving\", 'm giving up', 'giving up .']\n"
          ]
        }
      ]
    }
  ]
}

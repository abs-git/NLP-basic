{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Engish_Tokenizing",
      "provenance": [],
      "collapsed_sections": [
        "bkSYqvxmtWLc"
      ],
      "authorship_tag": "ABX9TyO4hbREgN5NIVU5uWntOSgF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/Engish_Tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OjauShm6ko0",
        "outputId": "ac1dd478-a5a7-4623-c17e-81b689ffa09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data load"
      ],
      "metadata": {
        "id": "bkSYqvxmtWLc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCNswxRbAiW2"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric"
      ],
      "metadata": {
        "id": "ezq5vc-ZV5rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
        "\n",
        "task = \"cola\"\n",
        "\n",
        "dataset = load_dataset(\"glue\", task)\n",
        "metric = load_metric(\"glue\", task)"
      ],
      "metadata": {
        "id": "Isiv0plWIH46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)\n",
        "print()\n",
        "print(dataset[\"train\"])\n",
        "print()\n",
        "print(dataset[\"train\"][0])"
      ],
      "metadata": {
        "id": "sDmxAGbbI4GB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"train\"][0]"
      ],
      "metadata": {
        "id": "YGOlnHpLJgTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data split and save\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "train_data = []\n",
        "for i, data in enumerate(dataset['train']):\n",
        "  train_data.append(dataset['train'][i]['sentence'])\n",
        "\n",
        "with open(path + '/glue_train.txt', 'w', encoding='utf-8') as f:\n",
        "  for sen in train_data:\n",
        "    f.write(sen + '\\n')\n",
        "\n",
        "\n",
        "test_data = []\n",
        "for i, data in enumerate(dataset['test']):\n",
        "  test_data.append(dataset['test'][i]['sentence'])\n",
        "\n",
        "with open(path + '/glue_test.txt', 'w', encoding='utf-8') as f:\n",
        "  for sen in test_data:\n",
        "    f.write(sen + '\\n')\n",
        "\n"
      ],
      "metadata": {
        "id": "zku5h3EeKkT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "svymZe03tbAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(len(train_sentences))\n",
        "print(len(test_sentences))\n",
        "\n",
        "print(train_sentences[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU3TN76GWIaV",
        "outputId": "2e570b8d-7af7-4376-e80d-d47c07c8136b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8551\n",
            "1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "v8BvQtLoKXdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df04c30-f5cd-4196-d258-2e9002512bbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Word Tokenization\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import spacy\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "sentences = train_sentences[:5]\n",
        "\n",
        "# nltk word_tokenize\n",
        "nltk_word_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = word_tokenize(sen)\n",
        "  nltk_word_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# nltk word punct\n",
        "nltk_punct_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = WordPunctTokenizer().tokenize(sen)               # 구두점을 별도로 분류한다.\n",
        "  nltk_punct_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# nltk Tree bank\n",
        "nltk_tree_bank_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = TreebankWordTokenizer().tokenize(sen)               # 하이픈(-)은 하나의 단어로, 아포스트로피(')는 접어로 취급한다.\n",
        "  nltk_tree_bank_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# spacy tokenize\n",
        "spacy_tokens = []\n",
        "spacy_en = spacy.load('en')\n",
        "for sen in sentences:\n",
        "  tokens = [tok.text for tok in spacy_en.tokenizer(sen)]\n",
        "  spacy_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# tensorflow keras\n",
        "keras_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = text_to_word_sequence(sen)               # 자동으로 lower가 진행되고 구두점(,.!)을 제거하지만, 아포스트로피(')는 보존한다.\n",
        "  keras_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# torch\n",
        "torch_tokenizer = get_tokenizer(\"basic_english\")\n",
        "torch_tokens = []\n",
        "for sen in sentences:\n",
        "  tokens = torch_tokenizer(sen)\n",
        "  torch_tokens.append(tokens)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Raw sentence :          \", sentences[0])\n",
        "print(\"nltk word tokens :      \", nltk_word_tokens[0])\n",
        "print(\"nltk punct tokens :     \", nltk_punct_tokens[0])\n",
        "print(\"nltk tree bank tokens : \", nltk_tree_bank_tokens[0])\n",
        "print(\"spacy tokens :          \", spacy_tokens[0])\n",
        "print(\"keras tokens :          \", keras_tokens[0])\n",
        "print(\"torch okens :           \", torch_tokens[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha7BQ_VbU7KH",
        "outputId": "88c7fff1-9606-4295-999a-dfd21587d578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw sentence :           Our friends won't buy this analysis, let alone the next one we propose.\n",
            "nltk word tokens :       ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "nltk punct tokens :      ['Our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "nltk tree bank tokens :  ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "spacy tokens :           ['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n",
            "keras tokens :           ['our', 'friends', \"won't\", 'buy', 'this', 'analysis', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose']\n",
            "torch okens :            ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "corpus = ' '.join(sentences)\n",
        "\n",
        "nltk_sent_tokens = sent_tokenize(corpus)            # 문장의 끝이 아닌 마침표(.)를 구분해서 분할\n",
        "\n",
        "\n",
        "print(\"corpus           : \", corpus)\n",
        "print(\"nltk sent tokens : \", nltk_sent_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQYqg9cnU_j3",
        "outputId": "4fb44bda-3c5d-425e-88aa-a2e20a7ff0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus           :  Our friends won't buy this analysis, let alone the next one we propose. One more pseudo generalization and I'm giving up. One more pseudo generalization or I'm giving up. The more we study verbs, the crazier they get. Day by day the facts are getting murkier.\n",
            "nltk sent tokens :  [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\", 'The more we study verbs, the crazier they get.', 'Day by day the facts are getting murkier.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword Tokenization\n",
        "\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "# BPE (Byte Pair Encoding)\n",
        "class BPE():\n",
        "  def __init__(self):\n",
        "    self.vocab = []\n",
        "\n",
        "\n",
        "  def make_dict(self, corpus):     # 코퍼스(corpus)를 읽어들여 글자(character)단위로 분리 후 딕셔너리(dict) 생성\n",
        "                                   # corpus는 sentence가 담긴 list\n",
        "    subword_dict = defaultdict(int)\n",
        "\n",
        "    for sen in corpus:\n",
        "      tokens = WordPunctTokenizer().tokenize(sen)\n",
        "\n",
        "\n",
        "      for token in tokens:\n",
        "        subword_dict[\" \".join(list(token)).lower() + \" </w>\"] = 1      # </w> 문자는 subword의 전/후면 위치를 구분 가능하도록 함.\n",
        "\n",
        "        self.vocab.extend(list(token.lower()))\n",
        "\n",
        "    return dict(subword_dict)\n",
        "\n",
        "\n",
        "  def get_pairs(self, subword_dict):          # dictionary 내의 bi-gram pair를 생성하고 빈도를 counting 한다. \n",
        "    \n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in subword_dict.items():\n",
        "      word_list = word.split()\n",
        "\n",
        "      for i in range(len(word_list) - 1):\n",
        "        pairs[(word_list[i], word_list[i+1])] += freq\n",
        "\n",
        "    return dict(pairs)\n",
        "\n",
        "\n",
        "  def merge_dict(self, pairs, subword_dict):\n",
        "\n",
        "    merged_subword_dict = defaultdict(int)\n",
        "    best_pair = max(pairs, key = pairs.get)   # bi-gram pair dictionary에 가장 많이 등장하는 \n",
        "\n",
        "    # print(best_pair)\n",
        "    for word, freq in subword_dict.items():\n",
        "      \n",
        "      if \" \".join(best_pair) in word:\n",
        "        merged_word = word.replace(\" \".join(best_pair), \"\".join(best_pair))\n",
        "\n",
        "        merged_subword_dict[merged_word] = subword_dict[word]    \n",
        "      else:\n",
        "        merged_subword_dict[word] = subword_dict[word]\n",
        "\n",
        "    return dict(merged_subword_dict)\n",
        "\n",
        "\n",
        "  def get_vocab(self, subword_dict):\n",
        "\n",
        "    temp_vocab = []\n",
        "    for word, freq in subword_dict.items():\n",
        "      word_list = word.split()\n",
        "\n",
        "      temp_vocab.extend(word_list)\n",
        "\n",
        "    self.vocab = list(set(temp_vocab) | set(self.vocab))\n",
        "\n",
        "    return self.vocab\n",
        "    \n"
      ],
      "metadata": {
        "id": "PtdQ2-wXU9UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "byte_pair_encoder = BPE()\n",
        "\n",
        "subword_dict = byte_pair_encoder.make_dict(sentences)\n",
        "print(subword_dict)\n",
        "print()\n",
        "\n",
        "for i in range(20):\n",
        "  pairs = byte_pair_encoder.get_pairs(subword_dict)\n",
        "  subword_dict = byte_pair_encoder.merge_dict(pairs, subword_dict)\n",
        "  vocab = byte_pair_encoder.get_vocab(subword_dict)\n",
        "\n",
        "print(subword_dict)\n",
        "print()\n",
        "print(vocab)\n",
        "print(len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1SgGspa8Ghy",
        "outputId": "f7b6a12c-6394-4b23-b689-81550de316b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'o u r </w>': 1, 'f r i e n d s </w>': 1, 'w o n </w>': 1, \"' </w>\": 1, 't </w>': 1, 'b u y </w>': 1, 't h i s </w>': 1, 'a n a l y s i s </w>': 1, ', </w>': 1, 'l e t </w>': 1, 'a l o n e </w>': 1, 't h e </w>': 1, 'n e x t </w>': 1, 'o n e </w>': 1, 'w e </w>': 1, 'p r o p o s e </w>': 1, '. </w>': 1, 'm o r e </w>': 1, 'p s e u d o </w>': 1, 'g e n e r a l i z a t i o n </w>': 1, 'a n d </w>': 1, 'i </w>': 1, 'm </w>': 1, 'g i v i n g </w>': 1, 'u p </w>': 1, 'o r </w>': 1, 's t u d y </w>': 1, 'v e r b s </w>': 1, 'c r a z i e r </w>': 1, 't h e y </w>': 1, 'g e t </w>': 1, 'd a y </w>': 1, 'b y </w>': 1, 'f a c t s </w>': 1, 'a r e </w>': 1, 'g e t t i n g </w>': 1, 'm u r k i e r </w>': 1}\n",
            "\n",
            "{'o u r</w>': 1, 'f r ie nd s</w>': 1, 'w on</w>': 1, \"' </w>\": 1, 't</w>': 1, 'b u y</w>': 1, 'th is</w>': 1, 'a n al y s is</w>': 1, ', </w>': 1, 'l e t</w>': 1, 'al one</w>': 1, 'th e</w>': 1, 'ne x t</w>': 1, 'one</w>': 1, 'w e</w>': 1, 'p r o p o s e</w>': 1, '. </w>': 1, 'm o re</w>': 1, 'p s e ud o </w>': 1, 'ge ne r al i z a ti on</w>': 1, 'a nd </w>': 1, 'i </w>': 1, 'm </w>': 1, 'g i v i ng</w>': 1, 'u p </w>': 1, 'o r</w>': 1, 's t ud y</w>': 1, 'v e r b s</w>': 1, 'c r a z ie r</w>': 1, 'th e y</w>': 1, 'ge t</w>': 1, 'd a y</w>': 1, 'b y</w>': 1, 'f a c t s</w>': 1, 'a re</w>': 1, 'ge t ti ng</w>': 1, 'm u r k ie r</w>': 1}\n",
            "\n",
            "['ge', ',', 'is</w>', 'o', 'm', 'a', 'ng', '</w>', 'i', 'th', 'r', 'ud', 'k', 's</w>', 'u', '.', 'nd', 'f', 'ti', 'on', 't', 'e</w>', 'h', 'p', 'ne', 'n', 'on</w>', 'al', 'one</w>', 'r</w>', 'g', 'd', 're</w>', 'l', \"'\", 'y', 'b', 'e', 'c', 'z', 'v', 'y</w>', 's', 'ng</w>', 'w', 't</w>', 'ie', 'x']\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data load\n",
        "# reference : https://colab.research.google.com/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb#scrollTo=UtFQqK3tmp7G\n",
        "# !wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "metadata": {
        "id": "uw37v7Ix-BIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SentencePiece (Unigram Language Model Tokenizer)\n",
        "\n",
        "import sentencepiece as spm\n",
        "import re\n",
        "\n",
        "train_data = './botchan.txt'\n",
        "\n",
        "# command setting\n",
        "templates = '--input={} --model_prefix={} --vocab_size={} --model_type={}'\n",
        "\n",
        "vocab_size = 5000\n",
        "prefix = './sentencepiece_prefix'\n",
        "model_type = 'bpe'\n",
        "\n",
        "# cmd = templates.format(train_sentences, prefix, vocab_size, model_type)       # 에러 발생....\n",
        "cmd = templates.format(train_data, prefix, vocab_size, model_type)\n",
        "\n",
        "\n",
        "# train\n",
        "spm.SentencePieceTrainer.Train(cmd)\n",
        "\n",
        "\n",
        "# model load\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load('{}.model'.format(prefix))\n",
        "\n",
        "with open('{}.vocab'.format(prefix), encoding='utf-8') as f:      # vocabrary 정보\n",
        "  vocab_info = [info.strip().split(\"\\t\") for info in f]\n",
        "\n",
        "\n",
        "# test\n",
        "tokens = sp.encode_as_pieces(test_sentences[0])    # Tokenizing\n",
        "tokens_IDs = sp.encode_as_ids(test_sentences[0])\n",
        "\n",
        "sen = sp.decode_pieces(tokens)                    # Detokenizing\n",
        "sen_from_IDs = sp.decode_ids(tokens_IDs)\n",
        "\n",
        "print(test_sentences[0])\n",
        "print(tokens)\n",
        "print(tokens_IDs)\n",
        "print(sen)\n",
        "print(sen_from_IDs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYlvlyQIobNg",
        "outputId": "a463dbd5-ba6a-4b3e-9825-6e5dc0b5c3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bill whistled past the house.\n",
            "['▁B', 'ill', '▁wh', 'ist', 'led', '▁past', '▁the', '▁house', '.']\n",
            "[180, 157, 105, 261, 447, 1563, 9, 278, 4951]\n",
            "Bill whistled past the house.\n",
            "Bill whistled past the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Huggingface tokenizer\n",
        "\n",
        "# !pip install tokenizers\n",
        "import os\n",
        "from tokenizers import CharBPETokenizer, BertWordPieceTokenizer, SentencePieceBPETokenizer, ByteLevelBPETokenizer\n",
        "\n",
        "# model load\n",
        "Basic_tokenizer = CharBPETokenizer()\n",
        "SP_tokenizer = SentencePieceBPETokenizer()\n",
        "BL_tokenizer = ByteLevelBPETokenizer()\n",
        "BWP_tokenizer = BertWordPieceTokenizer(strip_accents= False, lowercase= False)  # strip_accents는 학습 시와 load 시 동일해야한다.\n",
        "\n",
        "\n",
        "# data load\n",
        "data_path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "# params\n",
        "corpus_file = [data_path + '/glue_train.txt']\n",
        "vocab_size = 32000\n",
        "limit_alphabet = 5000\n",
        "output_path = data_path + '/output'\n",
        "min_freq = 10\n",
        "\n",
        "\n",
        "# train\n",
        "Basic_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "SP_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "BL_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "BWP_tokenizer.train(files = corpus_file,\n",
        "                      vocab_size = vocab_size,\n",
        "                      min_frequency = min_freq,\n",
        "                      limit_alphabet = limit_alphabet,\n",
        "                      show_progress = True\n",
        "                      )\n",
        "\n",
        "\n",
        "# model(vocab) save\n",
        "# Basic_tokenizer.save_model(data_path)\n",
        "# SP_tokenizer.save_model(data_path)\n",
        "# BL_tokenizer.save_model(data_path)\n",
        "# BWP_tokenizer.save_model(data_path)\n",
        "\n",
        "\n",
        "# model(vocab) load\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# test\n",
        "Basic_output = Basic_tokenizer.encode(test_sentences[0])\n",
        "SP_output = SP_tokenizer.encode(test_sentences[0])\n",
        "BL_output = BL_tokenizer.encode(test_sentences[0])\n",
        "BWP_output = BWP_tokenizer.encode(test_sentences[0])\n",
        "\n",
        "basic_decoded = Basic_tokenizer.decode(Basic_output.ids)\n",
        "SP_decoded = SP_tokenizer.decode(SP_output.ids)\n",
        "BL_decoded = BL_tokenizer.decode(BL_output.ids)\n",
        "BWP_decoded = BWP_tokenizer.decode(BWP_output.ids)\n",
        "\n",
        "\n",
        "print(\"input   : {}\".format(test_sentences[0]))\n",
        "print()\n",
        "\n",
        "print(\"basic tokens  : {}\".format(Basic_output.tokens))\n",
        "print(\"basic id      : {}\".format(Basic_output.ids))\n",
        "print(\"basic offsets : {}\".format(Basic_output.offsets))\n",
        "print(\"basic decode  : {}\".format(basic_decoded))\n",
        "print()\n",
        "\n",
        "print(\"SP tokens  : {}\".format(SP_output.tokens))\n",
        "print(\"SP id      : {}\".format(SP_output.ids))\n",
        "print(\"SP offsets : {}\".format(SP_output.offsets))\n",
        "print(\"SP decode  : {}\".format(SP_decoded))\n",
        "print()\n",
        "\n",
        "print(\"BL tokens  : {}\".format(BL_output.tokens))\n",
        "print(\"BL id      : {}\".format(BL_output.ids))\n",
        "print(\"BL offsets : {}\".format(BL_output.offsets))\n",
        "print(\"BL decode  : {}\".format(BL_decoded))\n",
        "print()\n",
        "\n",
        "print(\"BWP tokens  : {}\".format(BWP_output.tokens))\n",
        "print(\"BWP id      : {}\".format(BWP_output.ids))\n",
        "print(\"BWP offsets : {}\".format(BWP_output.offsets))\n",
        "print(\"BWP decode  : {}\".format(BWP_decoded))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtoHBvnLgw9P",
        "outputId": "fc167422-ec2b-464f-e6d4-a242e8c28a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input   : Bill whistled past the house.\n",
            "\n",
            "basic tokens  : ['Bill</w>', 'wh', 'ist', 'led</w>', 'p', 'ast</w>', 'the</w>', 'house</w>', '.</w>']\n",
            "basic id      : [232, 189, 724, 362, 68, 1123, 153, 689, 116]\n",
            "basic offsets : [(0, 4), (5, 7), (7, 10), (10, 13), (14, 15), (15, 18), (19, 22), (23, 28), (28, 29)]\n",
            "basic decode  : Bill whistled past the house .\n",
            "\n",
            "SP tokens  : ['▁Bill', '▁wh', 'ist', 'led', '▁p', 'ast', '▁the', '▁house', '.']\n",
            "SP id      : [197, 215, 374, 777, 105, 388, 89, 746, 11]\n",
            "SP offsets : [(0, 4), (4, 7), (7, 10), (10, 13), (13, 15), (15, 18), (18, 22), (22, 28), (28, 29)]\n",
            "SP decode  : Bill whistled past the house.\n",
            "\n",
            "BL tokens  : ['Bill', 'Ġwh', 'ist', 'led', 'Ġp', 'ast', 'Ġthe', 'Ġhouse', '.']\n",
            "BL id      : [567, 374, 501, 641, 274, 528, 259, 898, 13]\n",
            "BL offsets : [(0, 4), (4, 7), (7, 10), (10, 13), (13, 15), (15, 18), (18, 22), (22, 28), (28, 29)]\n",
            "BL decode  : Bill whistled past the house.\n",
            "\n",
            "BWP tokens  : ['Bill', 'whis', '##tle', '##d', 'past', 'the', 'house', '.']\n",
            "BWP id      : [215, 952, 2188, 104, 2055, 140, 743, 13]\n",
            "BWP offsets : [(0, 4), (5, 9), (9, 12), (12, 13), (14, 18), (19, 22), (23, 28), (28, 29)]\n",
            "BWP decode  : Bill whistled past the house.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Subword text encoder (tensorflow)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# model load\n",
        "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(train_sentences, target_vocab_size = 30000)\n",
        "\n",
        "tokens_sen = tokenizer.encode(test_sentences[0])\n",
        "original_sen = tokenizer.decode(tokens_sen)\n",
        "\n",
        "print(tokenizer.subwords[:10])\n",
        "print()\n",
        "\n",
        "print(test_sentences[0])\n",
        "print(tokens_sen)\n",
        "print(original_sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w68b-jBoFwNP",
        "outputId": "fbed86ab-2695-429a-988d-6287c5d85834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the_', 'to_', 'The_', 'a_', 'I_', 'that_', 'is_', ', ', 'John_', 'of_']\n",
            "\n",
            "Bill whistled past the house.\n",
            "[23, 3032, 2527, 1, 359, 7681]\n",
            "Bill whistled past the house.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B7Yp4L6goWC1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}

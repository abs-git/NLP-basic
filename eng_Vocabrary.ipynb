{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English_Vocabrary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOf+X6ajx2FpoQHb9+/gpgY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abs-git/NLP/blob/main/English_Vocabrary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72ATKcdYDTES",
        "outputId": "11eb8b21-001c-4ed9-c753-952ea9d9b30b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# glue data load\n",
        "\n",
        "path = '/content/gdrive/MyDrive/Colab Notebooks/NLP'\n",
        "\n",
        "with open(path + '/glue_train.txt', 'r') as f:\n",
        "  train_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(train_sentences):\n",
        "  train_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "with open(path + '/glue_test.txt', 'r') as f:\n",
        "  test_sentences = f.readlines()\n",
        "\n",
        "for i, sen in enumerate(test_sentences):\n",
        "  test_sentences[i] = sen.rstrip(\"\\n\")\n",
        "\n",
        "print(\"train : {}\".format(len(train_sentences)))\n",
        "print(\"test : {}\".format(len(test_sentences)))\n",
        "\n",
        "print(train_sentences[:2])\n",
        "print(test_sentences[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C12Jici9ELST",
        "outputId": "829a3314-4472-4a67-ae22-f35e338ebe94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train : 8551\n",
            "test : 1063\n",
            "[\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\"]\n",
            "['Bill whistled past the house.', 'The car honked its way down the road.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lDyKFpRGfYt",
        "outputId": "11e22eba-de2b-4820-e00d-31f82fb91426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing & cleaning\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Word tokenizing\n",
        "sen_to_tokens = []\n",
        "for sen in train_sentences[:3]:\n",
        "  sen_to_tokens.append(TreebankWordTokenizer().tokenize(sen))\n",
        "\n",
        "print(sen_to_tokens)\n",
        "\n",
        "\n",
        "# cleaning\n",
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "clean_tokens = []\n",
        "for sen in sen_to_tokens:\n",
        "  for t in sen:\n",
        "    if t not in stopwords:\n",
        "      clean_tokens.append(t)\n",
        "\n",
        "print(clean_tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avPBf44MKzSq",
        "outputId": "e4976ad4-9195-4070-d73e-7e06583d0ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'], ['One', 'more', 'pseudo', 'generalization', 'and', 'I', \"'m\", 'giving', 'up', '.'], ['One', 'more', 'pseudo', 'generalization', 'or', 'I', \"'m\", 'giving', 'up', '.']]\n",
            "['Our', 'friends', 'wo', \"n't\", 'buy', 'analysis', ',', 'let', 'alone', 'next', 'one', 'propose', '.', 'One', 'pseudo', 'generalization', 'I', \"'m\", 'giving', '.', 'One', 'pseudo', 'generalization', 'I', \"'m\", 'giving', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabrary\n",
        "from nltk import FreqDist\n",
        "\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# 단어들의 빈도수를 계산한다.\n",
        "vocab = FreqDist(np.hstack(clean_tokens)) \n",
        "vocab = dict(vocab)\n",
        "\n",
        "\n",
        "# 빈도수가 높은 단어들만 원하는 수 (vocab size) 만큼 불러온다.\n",
        "vocab_size = 10\n",
        "filtered_vocab = Counter(vocab).most_common(vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "# token에 index 부여\n",
        "word_to_index = {}\n",
        "word_to_index['pad'] = 0                  # padding을 위해 'pad'를 생성\n",
        "word_to_index['unk'] = 1                  # vocab에 없는 token은 'unk'\n",
        "\n",
        "for index, word in enumerate(filtered_vocab):\n",
        "  word_to_index[word[0]] = index + 2\n",
        "\n",
        "\n",
        "\n",
        "# Sentence를 index 조합으로 구성\n",
        "sen_to_index = []\n",
        "for sen in sen_to_tokens:\n",
        "  temp = []\n",
        "  for token in sen:\n",
        "    try:\n",
        "      temp.append(word_to_index[token])\n",
        "    except:\n",
        "      temp.append(word_to_index['unk'])\n",
        "\n",
        "  sen_to_index.append(temp)\n",
        "\n",
        "# padding\n",
        "max_len = 0\n",
        "for sen in sen_to_index:\n",
        "  if max_len <= len(sen):\n",
        "    max_len = len(sen)\n",
        "\n",
        "for sen in sen_to_index:\n",
        "  if len(sen) < max_len:\n",
        "    sen += [word_to_index['pad']] * (max_len - len(sen))\n",
        "\n",
        "\n",
        "\n",
        "print('vocab freq      : {}'.format(vocab))\n",
        "print('vocab filtering : {}'.format(filtered_vocab))\n",
        "print('word to index   : {}'.format(word_to_index))\n",
        "print('sentence        : {}'.format(sen_to_tokens))\n",
        "print('sen to index    : {}'.format(sen_to_index))\n",
        "print('padded sentence : {}'.format(sen_to_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFUyrLAcETY9",
        "outputId": "4670c97e-159d-49b3-bf47-40aeae97c07c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab freq      : {'Our': 1, 'friends': 1, 'wo': 1, \"n't\": 1, 'buy': 1, 'analysis': 1, ',': 1, 'let': 1, 'alone': 1, 'next': 1, 'one': 1, 'propose': 1, '.': 3, 'One': 2, 'pseudo': 2, 'generalization': 2, 'I': 2, \"'m\": 2, 'giving': 2}\n",
            "vocab filtering : [('.', 3), ('One', 2), ('pseudo', 2), ('generalization', 2), ('I', 2), (\"'m\", 2), ('giving', 2), ('Our', 1), ('friends', 1), ('wo', 1)]\n",
            "word to index   : {'pad': 0, 'unk': 1, '.': 2, 'One': 3, 'pseudo': 4, 'generalization': 5, 'I': 6, \"'m\": 7, 'giving': 8, 'Our': 9, 'friends': 10, 'wo': 11}\n",
            "sentence        : [['Our', 'friends', 'wo', \"n't\", 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.'], ['One', 'more', 'pseudo', 'generalization', 'and', 'I', \"'m\", 'giving', 'up', '.'], ['One', 'more', 'pseudo', 'generalization', 'or', 'I', \"'m\", 'giving', 'up', '.']]\n",
            "sen to index    : [[9, 10, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], [3, 1, 4, 5, 1, 6, 7, 8, 1, 2, 0, 0, 0, 0, 0, 0], [3, 1, 4, 5, 1, 6, 7, 8, 1, 2, 0, 0, 0, 0, 0, 0]]\n",
            "padded sentence : [[9, 10, 11, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2], [3, 1, 4, 5, 1, 6, 7, 8, 1, 2, 0, 0, 0, 0, 0, 0], [3, 1, 4, 5, 1, 6, 7, 8, 1, 2, 0, 0, 0, 0, 0, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tensorflow(keras) vocabrary\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token = 'OOV')           # num_words는 vocab size이고, oov_token은 위의 'unk'와 같다.\n",
        "tokenizer.fit_on_texts(train_sentences[:3])                         # train sentences에 대한 문장들 내의 단어들에 인덱스를 부여한다. 구두점 (, . ! ?)은 제외된다.\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "train_sen_to_index= tokenizer.texts_to_sequences(train_sentences[:3])        # train setences를 index 조합으로 구성한다.\n",
        "padded_train_sen = pad_sequences(train_sen_to_index, padding = 'post')      # padding\n",
        "\n",
        "test_sen_to_index = tokenizer.texts_to_sequences(test_sentences[:3])\n",
        "\n",
        "\n",
        "print(\"train sentences              : {}\".format(train_sentences[:3]))\n",
        "print(\"test sentences               : {}\".format(test_sentences[:3]))\n",
        "print()\n",
        "\n",
        "print(\"word index                   : {}\".format(word_index))\n",
        "print(\"train sentences to index seq : {}\".format(train_sen_to_index))\n",
        "print(\"test sentences to index seq  : {}\".format(test_sen_to_index))\n",
        "print()\n",
        "\n",
        "print(\"padded train sentences : {}\".format(padded_train_sen))\n",
        "print()\n",
        "\n",
        "print(\"Encoding samples\")\n",
        "print(train_sentences[0], \"->\", train_sen_to_index[0])\n",
        "print(test_sentences[0], \"->\", test_sen_to_index[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DDB_fhhaPSVC",
        "outputId": "3ec1ebd0-68cf-47cf-8b1a-85a65b2a3ebc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train sentences              : [\"Our friends won't buy this analysis, let alone the next one we propose.\", \"One more pseudo generalization and I'm giving up.\", \"One more pseudo generalization or I'm giving up.\"]\n",
            "test sentences               : ['Bill whistled past the house.', 'The car honked its way down the road.', 'Bill pushed Harry off the sofa.']\n",
            "\n",
            "word index                   : {'OOV': 1, 'one': 2, 'more': 3, 'pseudo': 4, 'generalization': 5, \"i'm\": 6, 'giving': 7, 'up': 8, 'our': 9, 'friends': 10, \"won't\": 11, 'buy': 12, 'this': 13, 'analysis': 14, 'let': 15, 'alone': 16, 'the': 17, 'next': 18, 'we': 19, 'propose': 20, 'and': 21, 'or': 22}\n",
            "train sentences to index seq : [[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20], [2, 3, 4, 5, 21, 6, 7, 8], [2, 3, 4, 5, 22, 6, 7, 8]]\n",
            "test sentences to index seq  : [[1, 1, 1, 17, 1], [17, 1, 1, 1, 1, 1, 17, 1], [1, 1, 1, 1, 17, 1]]\n",
            "\n",
            "padded train sentences : [[ 9 10 11 12 13 14 15 16 17 18  2 19 20]\n",
            " [ 2  3  4  5 21  6  7  8  0  0  0  0  0]\n",
            " [ 2  3  4  5 22  6  7  8  0  0  0  0  0]]\n",
            "\n",
            "Encoding samples\n",
            "Our friends won't buy this analysis, let alone the next one we propose. -> [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 2, 19, 20]\n",
            "Bill whistled past the house. -> [1, 1, 1, 17, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torchtext 0.12.0 version\n",
        "\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "tokens = []\n",
        "for sen in train_sentences[:3]:\n",
        "  sen_to_tokens = tokenizer(sen)\n",
        "  tokens.extend(sen_to_tokens)\n",
        "\n",
        "tokens_counter = Counter(tokens)\n",
        "\n",
        "sorted_tokens_dict = sorted(tokens_counter.items(), key = lambda x : x[1], reverse=True)\n",
        "\n",
        "ordered_dict = OrderedDict(sorted_tokens_dict)\n",
        "\n",
        "vocab = vocab(ordered_dict, specials=['pad','unk'])       # 'pad' = 0, 'unk' = 1\n",
        "\n",
        "token_to_index = vocab.get_stoi()\n",
        "token_to_index = sorted(token_to_index.items(), key = lambda x : x[1])\n",
        "\n",
        "\n",
        "print('tokens         : {}'.format(tokens))\n",
        "print('sorted by freq : {}'.format(sorted_tokens_dict))\n",
        "print('ordered dict   : {}'.format(ordered_dict))\n",
        "print('token to index : {}'.format(token_to_index))\n"
      ],
      "metadata": {
        "id": "CErab5HdqWp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "503ca4d2-9441-40a0-9ab3-fefe7ab14145"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens         : ['our', 'friends', 'won', \"'\", 't', 'buy', 'this', 'analysis', ',', 'let', 'alone', 'the', 'next', 'one', 'we', 'propose', '.', 'one', 'more', 'pseudo', 'generalization', 'and', 'i', \"'\", 'm', 'giving', 'up', '.', 'one', 'more', 'pseudo', 'generalization', 'or', 'i', \"'\", 'm', 'giving', 'up', '.']\n",
            "sorted by freq : [(\"'\", 3), ('one', 3), ('.', 3), ('more', 2), ('pseudo', 2), ('generalization', 2), ('i', 2), ('m', 2), ('giving', 2), ('up', 2), ('our', 1), ('friends', 1), ('won', 1), ('t', 1), ('buy', 1), ('this', 1), ('analysis', 1), (',', 1), ('let', 1), ('alone', 1), ('the', 1), ('next', 1), ('we', 1), ('propose', 1), ('and', 1), ('or', 1)]\n",
            "ordered dict   : OrderedDict([(\"'\", 3), ('one', 3), ('.', 3), ('more', 2), ('pseudo', 2), ('generalization', 2), ('i', 2), ('m', 2), ('giving', 2), ('up', 2), ('our', 1), ('friends', 1), ('won', 1), ('t', 1), ('buy', 1), ('this', 1), ('analysis', 1), (',', 1), ('let', 1), ('alone', 1), ('the', 1), ('next', 1), ('we', 1), ('propose', 1), ('and', 1), ('or', 1)])\n",
            "token to index : [('pad', 0), ('unk', 1), (\"'\", 2), ('one', 3), ('.', 4), ('more', 5), ('pseudo', 6), ('generalization', 7), ('i', 8), ('m', 9), ('giving', 10), ('up', 11), ('our', 12), ('friends', 13), ('won', 14), ('t', 15), ('buy', 16), ('this', 17), ('analysis', 18), (',', 19), ('let', 20), ('alone', 21), ('the', 22), ('next', 23), ('we', 24), ('propose', 25), ('and', 26), ('or', 27)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HJbwBMov_mR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
